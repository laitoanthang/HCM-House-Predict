{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 1: DATA SCIENCE\n",
    "1. DATA COLLECTION (THU THẬP DỮ LIỆU)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT NECESSARY LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATE A SCRAPY PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'colect_data', using template directory 'C:\\Users\\laito\\anaconda3\\lib\\site-packages\\scrapy\\templates\\project', created in:\n",
      "    c:\\Users\\laito\\OneDrive - VNU-HCMUS\\Jupyter Hub\\CNTT-NMDS\\HCM-House-Predict\\Data Science Process\\Data Crawl\\colect_data\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd colect_data\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject colect_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 3] The system cannot find the path specified: 'colect_data/colect_data'\n",
      "c:\\Users\\laito\\OneDrive - VNU-HCMUS\\Jupyter Hub\\CNTT-NMDS\\HCM-House-Predict\\Data Science Process\\Data Crawl\\colect_data\\colect_data\n"
     ]
    }
   ],
   "source": [
    "cd colect_data/colect_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sau khi tạo xong project với scrapy, ta cần tạo một file có tên collect_data.py vào đường dẫn sau collect_data/collect_data/spiders/collect_data.py. Sau đó chạy lệnh sau để thu thập dữ liệu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-29 08:14:30 [scrapy.utils.log] INFO: Scrapy 2.6.2 started (bot: colect_data)\n",
      "2023-01-29 08:14:30 [scrapy.utils.log] INFO: Versions: lxml 4.9.1.0, libxml2 2.9.14, cssselect 1.1.0, parsel 1.6.0, w3lib 1.21.0, Twisted 22.2.0, Python 3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 22.0.0 (OpenSSL 1.1.1q  5 Jul 2022), cryptography 37.0.1, Platform Windows-10-10.0.19045-SP0\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\laito\\anaconda3\\lib\\site-packages\\scrapy\\spiderloader.py\", line 75, in load\n",
      "    return self._spiders[spider_name]\n",
      "KeyError: 'Get_Data'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\laito\\anaconda3\\Scripts\\scrapy-script.py\", line 10, in <module>\n",
      "    sys.exit(execute())\n",
      "  File \"C:\\Users\\laito\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 154, in execute\n",
      "    _run_print_help(parser, _run_command, cmd, args, opts)\n",
      "  File \"C:\\Users\\laito\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 109, in _run_print_help\n",
      "    func(*a, **kw)\n",
      "  File \"C:\\Users\\laito\\anaconda3\\lib\\site-packages\\scrapy\\cmdline.py\", line 162, in _run_command\n",
      "    cmd.run(args, opts)\n",
      "  File \"C:\\Users\\laito\\anaconda3\\lib\\site-packages\\scrapy\\commands\\crawl.py\", line 22, in run\n",
      "    crawl_defer = self.crawler_process.crawl(spname, **opts.spargs)\n",
      "  File \"C:\\Users\\laito\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 204, in crawl\n",
      "    crawler = self.create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\laito\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 237, in create_crawler\n",
      "    return self._create_crawler(crawler_or_spidercls)\n",
      "  File \"C:\\Users\\laito\\anaconda3\\lib\\site-packages\\scrapy\\crawler.py\", line 312, in _create_crawler\n",
      "    spidercls = self.spider_loader.load(spidercls)\n",
      "  File \"C:\\Users\\laito\\anaconda3\\lib\\site-packages\\scrapy\\spiderloader.py\", line 77, in load\n",
      "    raise KeyError(f\"Spider not found: {spider_name}\")\n",
      "KeyError: 'Spider not found: Get_Data'\n"
     ]
    }
   ],
   "source": [
    "!scrapy crawl Get_Data -o dataset/inf.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5c043fe8d917de58502c0ff917ef6a8bc6988f96fc7225ef9721be17873a29ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
